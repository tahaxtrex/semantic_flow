{
  "course_metadata": {
    "title": "Dsa",
    "author": "Unknown",
    "target_audience": "Unknown",
    "subject": "Unknown",
    "source": "Dsa.pdf",
    "description": "Unknown",
    "prerequisites": [],
    "learning_outcomes": []
  },
  "overall_score": {
    "goal_focus": 9.0,
    "text_readability": 8.0,
    "pedagogical_clarity": 8.0,
    "prerequisite_alignment": 7.0,
    "fluidity_continuity": 9.0,
    "structural_usability": 8.0,
    "example_concreteness": 6.0,
    "example_coherence": 7.0,
    "business_relevance": 9.0,
    "instructional_alignment": 8.0
  },
  "segments": [
    {
      "segment_id": 1,
      "heading": "Annotated Reference with Examples",
      "text": "First Edition\nCopyright [?] c Granville Barnett, and Luca Del Tongo 2008.\nThis book is made exclusively available from DotNetSlackers\n( http://dotnetslackers.com/ ) the place for .NET articles, and news from\nsome of the leading minds in the software industry.\n\n1 Introduction 1\n1.1 What this book is, and what it isn’t . . . . . . . . . . . . . . . . 1\n1.2 Assumed knowledge . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2.1 Big Oh notation . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2.2 Imperative programming language . . . . . . . . . . . . . 3\n1.2.3 Object oriented concepts . . . . . . . . . . . . . . . . . . 4\n1.3 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.4 Tips for working through the examples . . . . . . . . . . . . . . . 6\n1.5 Book outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.6 Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.7 Where can I get the code? . . . . . . . . . . . . . . . . . . . . . . 7\n1.8 Final messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\nI Data Structures 8\n2 Linked Lists 9\n2.1 Singly Linked List . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.1.1 Insertion. . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.1.2 Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.1.3 Deletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.1.4 Traversing the list . . . . . . . . . . . . . . . . . . . . . . 12\n2.1.5 Traversing the list in reverse order . . . . . . . . . . . . . 13\n2.2 Doubly Linked List . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.1 Insertion. . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.2 Deletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.3 Reverse Traversal . . . . . . . . . . . . . . . . . . . . . . . 16\n2.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3 Binary Search Tree 19\n3.1 Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.2 Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.3 Deletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.4 Finding the parent of a given node . . . . . . . . . . . . . . . . . 24\n3.5 Attaining a reference to a node . . . . . . . . . . . . . . . . . . . 24\n3.6 Finding the smallest and largest values in the binary search tree 25\n3.7 Tree Traversals . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.7.1 Preorder . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nI\n3.7.2 Postorder . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.7.3 Inorder . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.7.4 Breadth First . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4 Heap 32\n4.1 Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.2 Deletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n4.3 Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n4.4 Traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n5 Sets 44\n5.1 Unordered . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.1.1 Insertion. . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.2 Ordered . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n5.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n6 Queues 48\n6.1 A standard queue . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.2 Priority Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.3 Double Ended Queue . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n7 AVL Tree 54\n7.1 Tree Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n7.2 Tree Rebalancing . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n7.3 Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7.4 Deletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n7.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\nII Algorithms 62\n8 Sorting 63\n8.1 Bubble Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n8.2 Merge Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n8.3 Quick Sort. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n8.4 Insertion Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n8.5 Shell Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n8.6 Radix Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n8.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n9 Numeric 72\n9.1 Primality Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n9.2 Base conversions . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n9.3 Attaining the greatest common denominator of two numbers . . 73\n9.4 Computing the maximum value for a number of a specific base\nconsisting of N digits . . . . . . . . . . . . . . . . . . . . . . . . . 74\n9.5 Factorial of a number . . . . . . . . . . . . . . . . . . . . . . . . 74\n9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\nII\n10 Searching 76\n10.1 Sequential Search . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n10.2 Probability Search . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n10.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n11 Strings 79\n11.1 Reversing the order of words in a sentence . . . . . . . . . . . . . 79\n11.2 Detecting a palindrome . . . . . . . . . . . . . . . . . . . . . . . 80\n11.3 Counting the number of words in a string . . . . . . . . . . . . . 81\n11.4 Determining the number of repeated words within a string . . . . 83\n11.5 Determining the first matching character between two strings . . 84\n11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nA Algorithm Walkthrough 86\nA.1 Iterative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 86\nA.2 Recursive Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . 88\nA.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\nB Translation Walkthrough 91\nB.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\nC Recursive Vs. Iterative Solutions 93\nC.1 Activation Records . . . . . . . . . . . . . . . . . . . . . . . . . . 94\nC.2 Some problems are recursive in nature . . . . . . . . . . . . . . . 95\nC.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nD Testing 97\nD.1 What constitutes a unit test? . . . . . . . . . . . . . . . . . . . . 97\nD.2 When should I write my tests? . . . . . . . . . . . . . . . . . . . 98\nD.3 How seriously should I view my test suite? . . . . . . . . . . . . . 99\nD.4 The three A’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\nD.5 The structuring of tests . . . . . . . . . . . . . . . . . . . . . . . 99\nD.6 Code Coverage . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\nD.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\nE Symbol Definitions 101\nIII",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 2,
      "heading": "Preface",
      "text": "Every book has a story as to how it came about and this one is no different,\nalthough we would be lying if we said its development had not been somewhat\nimpromptu. Put simply this book is the result of a series of emails sent back\nand forth between the two authors during the development of a library for\nthe .NET framework of the same name (with the omission of the subtitle of\ncourse!). The conversation started off something like, “Why don’t we create\na more aesthetically pleasing way to present our pseudocode?” After a few\nweeks this new presentation style had in fact grown into pseudocode listings\nwith chunks of text describing how the data structure or algorithm in question\nworks and various other things about it. At this point we thought, “What the\nheck, let’s make this thing into a book!” And so, in the summer of 2008 we\nbegan work on this book side by side with the actual library implementation.\nWhen we started writing this book the only things that we were sure about\nwith respect to how the book should be structured were:\n1. alwaysmakeexplanationsassimpleaspossiblewhilemaintainingamoder-\natelyfinedegreeofprecisiontokeepthemoreeagermindedreaderhappy;\nand\n2. injectdiagramstodemystifyproblemsthatareevenmoderatlychallenging\ntovisualise(...andsowecouldrememberhowourownalgorithmsworked\nwhen looking back at them!); and finally\n3. presentconciseandself-explanatorypseudocodelistingsthatcanbeported\neasily to most mainstream imperative programming languages like C++,\nC#, and Java.\nA key factor of this book and its associated implementations is that all\nalgorithms (unless otherwise stated) were designed by us, using the theory of\nthe algorithm in question as a guideline (for which we are eternally grateful to\ntheir original creators). Therefore they may sometimes turn out to be worse\nthan the “normal” implementations—and sometimes not. We are two fellows\nof the opinion that choice is a great thing. Read our book, read several others\non the same subject and use what you see fit from each (if anything) when\nimplementing your own version of the algorithms in question.\nThroughthisbookwehopethatyouwillseetheabsolutenecessityofunder-\nstanding which data structure or algorithm to use for a certain scenario. In all\nprojects, especially those that are concerned with performance (here we apply\nan even greater emphasis on real-time systems) the selection of the wrong data\nstructure or algorithm can be the cause of a great deal of performance pain.\nIV\nV\nThereforeitisabsolutelykeythatyouthinkabouttheruntimecomplexityand\nspace requirements of your selected approach. In this book we only explain the\ntheoreticalimplications to consider, but this is fora good reason: compilers are\nvery different in how they work. One C++ compiler may have some amazing\noptimisation phases specifically targeted at recursion, another may not, for ex-\nample. Of course this is just an example but you would be surprised by how\nmany subtle differences there are between compilers. These differences which\nmay make a fast algorithm slow, and vice versa. We could also factor in the\nsame concerns about languages that target virtual machines, leaving all the\nactual various implementation issues to you given that you will know your lan-\nguage’s compiler much better than us...well in most cases. This has resulted in\na more concise book that focuses on what we think are the key issues.\nOne final note: never take the words of others as gospel; verify all that can\nbe feasibly verified and make up your own mind.\nWehopeyouenjoyreadingthisbookasmuchaswehaveenjoyedwritingit.\nGranville Barnett\nLuca Del Tongo\n\nWriting this short book has been a fun and rewarding experience. We would\nlike to thank, in no particular order the following people who have helped us\nduring the writing of this book.\nSonu Kapoor generously hosted our book which when we released the first\ndraft received over thirteen thousand downloads, without his generosity this\nbook wouldnot havebeen ableto reachso manypeople. Jon Skeetprovidedus\nwith an alarming number of suggestions throughout for which we are eternally\ngrateful. Jon also edited this book as well.\nWewouldalsoliketothankthosewhoprovidedtheoddsuggestionviaemail\nto us. All feedback was listened to and you will no doubt see some content\ninfluenced by your suggestions.\nA special thank you also goes out to those who helped publicise this book\nfrom Microsoft’s Channel 9 weekly show (thanks Dan!) to the many bloggers\nwho helped spread the word. You gave us an audience and for that we are\nextremely grateful.\nThank you to all who contributed in some way to this book. The program-\nming community never ceases to amaze us in howwilling its constituentsare to\ngive time to projects such as this one. Thank you.\nVI\n\nGranvilleiscurrentlyaPh.DcandidateatQueenslandUniversityofTechnology\n(QUT)workingonparallelismattheMicrosoftQUTeResearchCentre . Healso\nholdsadegreeinComputerScience,andisaMicrosoftMVP.Hismaininterests\nare in programming languages and compilers. Granville can be contacted via\none of two places: either his personal website ( http://gbarnett.org ) or his\nblog ( http://msmvps.com/blogs/gbarnett ).\n\nLuca is currently studying for his masters degree in Computer Science at Flo-\nrence. His main interests vary from web development to research fields such as\ndata mining and computer vision. Luca also maintains an Italian blog which\ncan be found at http://blogs.ugidotnet.org/wetblog/ .\nhttp://www.mquter.qut.edu.au/\nVII\nPage intentionally left blank.\n\nThis book provides implementations of common and uncommon algorithms in\npseudocodewhichislanguageindependentandprovidesforeasyportingtomost\nimperative programming languages. It is not a definitive book on the theory of\ndata structures and algorithms.\nForthemostpartthisbookpresentsimplementationsdevisedbytheauthors\nthemselves based on the concepts by which the respective algorithms are based\nupon so it is more than possible that our implementations differ from those\nconsidered the norm.\nYou should use this book alongside another on the same subject, but one\nthat contains formal proofs of the algorithms in question. In this book we use\nthe abstract big Oh notation to depict the run time complexity of algorithms\nso that the book appeals to a larger audience.",
      "segment_type": "instructional",
      "scores": {
        "goal_focus": 9,
        "text_readability": 8,
        "pedagogical_clarity": 8,
        "prerequisite_alignment": 7,
        "fluidity_continuity": 9,
        "structural_usability": 8,
        "example_concreteness": 6,
        "example_coherence": 7,
        "business_relevance": 9,
        "instructional_alignment": 8
      },
      "reasoning": {
        "goal_focus_rationale": "The preface is tightly focused on introducing the book's purpose, philosophy, and scope. It clearly outlines what the reader can expect, including the authors' approach to algorithms, the emphasis on practical implications, and the book's limitations (not a definitive theory book). There are no significant digressions.",
        "text_readability_rationale": "The language used is generally clear, conversational, and accessible, appropriate for a preface. While there are some formatting issues (e.g., concatenated words), these are noted as PDF extraction artifacts and not penalized. The prose is grammatically correct and easy to follow.",
        "pedagogical_clarity_rationale": "As a preface, the segment's pedagogical clarity lies in explaining the book's instructional approach. It clearly states the principles (simplicity, diagrams, concise pseudocode) and the decision to focus on theoretical implications rather than compiler specifics. Terms like 'pseudocode' and 'Big Oh notation' are mentioned in context, assuming a baseline understanding for the target audience of a DSA book, or clarifying how the book will use them.",
        "prerequisite_alignment_rationale": "Although the course metadata states 'Prerequisites: None specified,' the preface itself clarifies the book's role by stating it is 'not a definitive book on the theory of data structures and algorithms' and recommends using it 'alongside another on the same subject, but one that contains formal proofs.' This explicitly sets expectations for the reader to either possess or acquire theoretical foundational knowledge, which is a good way to align prerequisites within the book's own context.",
        "fluidity_continuity_rationale": "The segment demonstrates excellent fluidity. It transitions smoothly from the book's origin story to its core principles, then to the authors' approach to implementations, the importance of the subject, acknowledgements, author bios, and a final summary of the book's nature. The flow is logical and coherent.",
        "structural_usability_rationale": "As a preface, the segment is a single, coherent introductory section. It clearly outlines the book's structural principles (e.g., 'always make explanations as simple as possible,' 'inject diagrams,' 'present concise and self-explanatory pseudocode listings'). The presence of a 'Heading: Preface' and implied sub-sections (acknowledgements, author bios) indicates a well-organized structure for an introductory piece.",
        "example_concreteness_rationale": "While the preface does not contain concrete examples of data structures or algorithms, it provides concrete context for the *relevance* of the subject by mentioning 'performance' and 'real-time systems' as scenarios where algorithm choice is critical. It also describes the *type* of examples the book will contain ('diagrams to demystify problems,' 'concise and self-explanatory pseudocode listings').",
        "example_coherence_rationale": "This segment, being a preface, does not present specific examples to evaluate for coherence. However, it establishes a consistent approach to how examples will be presented throughout the book (e.g., 'inject diagrams to demystify problems,' 'present concise and self-explanatory pseudocode listings'), thereby setting a foundation for future example coherence.",
        "business_relevance_rationale": "The preface strongly emphasizes the practical and business relevance of understanding data structures and algorithms. It highlights the 'absolute necessity of understanding which data structure or algorithm to use for a certain scenario' and directly links poor choices to 'a great deal of performance pain,' especially in 'real-time systems.' This clearly addresses potential performance gaps and offers actionable takeaways regarding critical thinking about runtime complexity and space requirements.",
        "instructional_alignment_rationale": "The preface clearly describes the instructional materials (pseudocode, diagrams) and the authors' pedagogical approach (simplicity, own implementations, focus on theoretical implications). It also sets appropriate expectations by clarifying the book's scope ('not a definitive book on the theory') and recommending its use alongside more formal resources, ensuring alignment between the book's content and the learner's overall learning journey."
      }
    },
    {
      "segment_id": 3,
      "heading": "1.2 Assumed knowledge",
      "text": "We have written this book with few assumptions of the reader, but some have\nbeennecessaryinordertokeepthebookasconciseandapproachableaspossible.\nWe assume that the reader is familiar with the following:\n1. Big Oh notation\n2. An imperative programming language\n3. Object oriented concepts\n1.2.1 Big Oh notation\nForruntimecomplexityanalysisweusebigOhnotationextensivelysoitisvital\nthat you are familiar with the general concepts to determine which is the best\nalgorithm for you in certain scenarios. We have chosen to use big Oh notation\nfor a few reasons, the most important of which is that it provides an abstract\nmeasurement by which we can judge the performance of algorithms without\nusing mathematical proofs.\nCHAPTER 1. INTRODUCTION 2\nFigure 1.1: Algorithmic run time expansion\nFigure1.1showssomeoftheruntimestodemonstratehowimportantitisto\nchooseanefficientalgorithm. Forthesanityofourgraphwehaveomittedcubic\n3 n\nO ( n ), and exponential O (2 ) run times. Cubic and exponential algorithms\nshouldonlyeverbeusedforverysmallproblems(ifever!);avoidthemiffeasibly\npossible.\nThe following list explains some of the most common big Oh notations:\nO (1) constant: theoperationdoesn’tdependonthesizeofitsinput,e.g. adding\na node to the tail of a linked list where we always maintain a pointer to\nthe tail node.\nO ( n ) linear: the run time complexity is proportionate to the size of n .\nO ( log n ) logarithmic: normally associated with algorithms that break the problem\ninto smaller chunks per each invocation, e.g. searching a binary search\ntree.\nO ( n log n ) just nlogn : usuallyassociatedwithanalgorithmthatbreakstheproblem\nintosmallerchunkspereachinvocation,andthentakestheresultsofthese\nsmaller chunks and stitches them back together, e.g. quick sort.\nO ( n 2 ) quadratic: e.g. bubble sort.\nO ( n 3 ) cubic: very rare.\nn\nO (2 ) exponential: incredibly rare.\nIfyouencountereitherofthelattertwoitems(cubicandexponential)thisis\nreally a signal for you to review the design of your algorithm. While prototyp-\ning algorithm designs you may just have the intention of solving the problem\nirrespective of how fast it works. We would strongly advise that you always\nreview your algorithm design and optimise where possible—particularly loops\nCHAPTER 1. INTRODUCTION 3\nand recursive calls—so that you can get the most efficient run times for your\nalgorithms.\nThe biggest asset that big Oh notation gives us is that it allows us to es-\nsentially discard things like hardware. If you have two sorting algorithms, one\nwith a quadratic run time, and the other with a logarithmic run time then the\nlogarithmic algorithm will always be faster than the quadratic one when the\ndata set becomes suitably large. This applies even if the former is ran on a ma-\nchine that is far faster than the latter. Why? Because big Oh notation isolates\na key factor in algorithm analysis: growth. An algorithm with a quadratic run\ntime grows faster than one with a logarithmic run time. It is generally said at\nsome point as n → ∞ the logarithmic algorithm will become faster than the\nquadratic algorithm.\nBig Oh notation also acts as a communication tool. Picture the scene: you\nare having a meeting with some fellow developers within your product group.\nYouarediscussingprototypealgorithmsfornodediscoveryinmassivenetworks.\nSeveral minutes elapse after you and two others have discussed your respective\nalgorithms and how they work. Does this give you a good idea of how fast each\nrespective algorithm is? No. The result of such a discussion will tell you more\naboutthehighlevelalgorithmdesignratherthanitsefficiency. Replaythescene\nback in your head, but this time as well as talking about algorithm design each\nrespective developer states the asymptotic run time of their algorithm. Using\nthe latter approach you not only get a good general idea about the algorithm\ndesign, but also key efficiency data which allows you to make better choices\nwhen it comes to selecting an algorithm fit for purpose.\nSome readers may actually work in a product group where they are given\nbudgets per feature. Each feature holds with it a budget that represents its up-\npermosttimebound. Ifyousavesometimeinonefeatureitdoesn’tnecessarily\ngive you a buffer for the remaining features. Imagine you are working on an\napplication, and you are in the team that is developing the routines that will\nessentially spin up everything that is required when the application is started.\nEverything is great until your boss comes in and tells you that the start up\ntime should not exceed n ms. The efficiency of every algorithm that is invoked\nduring start up in this example is absolutely key to a successful product. Even\nif you don’t have these budgets you should still strive for optimal solutions.\nTaking a quantitative approach for many software development properties\nwill make you a far superior programmer - measuring one’s work is critical to\nsuccess.\n1.2.2 Imperative programming language\nAll examples are given in a pseudo-imperative coding format and so the reader\nmust know the basics of some imperative mainstream programming language\nto port the examples effectively, we have written this book with the following\ntarget languages in mind:\n1. C++\n2. C#\n3. Java\nCHAPTER 1. INTRODUCTION 4\nThe reason that we are explicit in this requirement is simple—all our imple-\nmentations are based on an imperative thinking style. If you are a functional\nprogrammeryouwillneedtoapplyvariousaspectsfromthefunctionalparadigm\nto produce efficient solutions with respect to your functional language whether\nit be Haskell, F#, OCaml, etc.\nTwo of the languages that we have listed (C# and Java) target virtual\nmachines which provide various things like security sand boxing, and memory\nmanagement via garbage collection algorithms. It is trivial to port our imple-\nmentations to these languages. When porting to C++ you must remember to\nuse pointers for certain things. For example, when we describe a linked list\nnode as having a reference to the next node, this description is in the context\nof a managed environment. In C++ you should interpret the reference as a\npointer to the next node and so on. For programmers who have a fair amount\nof experience with their respective language these subtleties will present no is-\nsue, which is why we really do emphasise that the reader must be comfortable\nwith at least one imperative language in order to successfully port the pseudo-\nimplementations in this book.\nIt is essential that the user is familiar with primitive imperative language\nconstructs before reading this book otherwise you will just get lost. Some algo-\nrithms presented in this book can be confusing to follow even for experienced\nprogrammers!\n1.2.3 Object oriented concepts\nFor the most part this book does not use features that are specific to any one\nlanguage. In particular, we never provide data structures or algorithms that\nwork on generic types—this is in order to make the samples as easy to follow\nas possible. However, to appreciate the designs of our data structures you will\nneed to be familiar with the following object oriented (OO) concepts:\n1. Inheritance\n2. Encapsulation\n3. Polymorphism\nThisisespeciallyimportantifyouareplanningonlookingattheC#target\nthat we have implemented (more on that in § 1.7) which makes extensive use\nof the OO concepts listed above. As a final note it is also desirable that the\nreader is familiar with interfaces as the C# target uses interfaces throughout\nthe sorting algorithms.",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 4,
      "heading": "1.3 Pseudocode",
      "text": "Throughout this book we use pseudocode to describe our solutions. For the\nmost part interpreting the pseudocode is trivial as it looks very much like a\nmore abstract C++, or C#, but there are a few things to point out:\n1. Pre-conditions should always be enforced\n2. Post-conditionsrepresenttheresultofapplyingalgorithm a todatastruc-\nture d\nCHAPTER 1. INTRODUCTION 5\n3. The type of parameters is inferred\n4. All primitive language constructs are explicitly begun and ended\nIf an algorithm has a return type it will often be presented in the post-\ncondition, but where the return type is sufficiently obvious it may be omitted\nfor the sake of brevity.\nMost algorithms in this book require parameters, and because we assign no\nexplicittypetothoseparametersthetypeisinferredfromthecontextsinwhich\nit is used, and the operations performed upon it. Additionally, the name of\nthe parameter usually acts as the biggest clue to its type. For instance n is a\npseudo-name for a number and so you can assume unless otherwise stated that\nn translates to an integer that has the same number of bits as a WORD on a\n32bitmachine,similarly l isapseudo-nameforalistwherealistisaresizeable\narray (e.g. a vector).\nThelastmajorpointofreferenceisthatwealwaysexplicitlyendalanguage\nconstruct. For instance if we wish to close the scope of a for loop we will\nexplicitly state end for rather than leaving the interpretation of when scopes\nareclosedtothereader. Whileimplicitscopeclosureworkswellinsimplecode,\nin complex cases it can lead to ambiguity.\nThepseudocodestylethatweusewithinthisbookisratherstraightforward.\nAll algorithms start with a simple algorithm signature, e.g.\n1) algorithm AlgorithmName( arg 1, arg 2, ..., argN )\n2) ...\nn) end AlgorithmName\nImmediately after the algorithm signature we list any Pre or Post condi-\ntions.\n1) algorithm AlgorithmName( n )\n2) Pre: n is the value to compute the factorial of\n3) n ≥ 0\n4) Post: the factorial of n has been computed\n5) // ...\nn) end AlgorithmName\nThe example above describes an algorithm by the name of AlgorithmName ,\nwhich takes a single numeric parameter n . The pre and post conditions follow\nthe algorithm signature; you should always enforce the pre-conditions of an\nalgorithm when porting them to your language of choice.\nNormallywhatislistedasapre-coniditioniscriticaltothealgorithmsopera-\ntion. Thismaycoverthingsliketheactualparameternotbeingnull,orthatthe\ncollection passed in must contain at least n items. The post-condition mainly\ndescribestheeffectofthealgorithmsoperation. Anexampleofapost-condition\nmight be “The list has been sorted in ascending order”\nBecause everything we describe is language independent you will need to\nmake your own mind up on how to best handle pre-conditions. For example,\nin the C# target we have implemented, we consider non-conformance to pre-\nconditions to be exceptional cases. We provide a message in the exception to\ntell the caller why the algorithm has failed to execute normally.\nCHAPTER 1. INTRODUCTION 6\n\nAs with most books you get out what you put in and so we recommend that in\norder to get the most out of this book you work through each algorithm with a\npen and paper to track things like variable names, recursive calls etc.\nThe best way to work through algorithms is to set up a table, and in that\ntablegiveeachvariableitsowncolumnandcontinuouslyupdatethesecolumns.\nThis will help you keep track of and visualise the mutations that are occurring\nthroughout the algorithm. Often while working through algorithms in such\na way you can intuitively map relationships between data structures rather\nthan trying to work out a few values on paper and the rest in your head. We\nsuggest you put everything on paper irrespective of how trivial some variables\nand calculations may be so that you always have a point of reference.\nWhen dealing with recursive algorithm traces we recommend you do the\nsame as the above, but also have a table that records function calls and who\ntheyreturnto. Thisapproachisafarcleanerwaythandrawingoutanelaborate\nmap of function calls with arrows to one another, which gets large quickly and\nsimply makes things more complex to follow. Track everything in a simple and\nsystematic way to make your time studying the implementations far easier.\n\nWe have split this book into two parts:\nPart 1: Provides discussion and pseudo-implementations of common and uncom-\nmon data structures; and\nPart 2: Providesalgorithmsofvaryingpurposesfromsortingtostringoperations.\nThe reader doesn’t have to read the book sequentially from beginning to\nend: chapters can be read independently from one another. We suggest that\nin part 1 you read each chapter in its entirety, but in part 2 you can get away\nwith just reading the section of a chapter that describes the algorithm you are\ninterested in.\nEachofthechaptersondatastructurespresentinitiallythealgorithmscon-\ncerned with:\n1. Insertion\n2. Deletion\n3. Searching\nThe previous list represents what we believe in the vast majority of cases to\nbe the most important for each respective data structure.\nFor all readers we recommend that before looking at any algorithm you\nquickly look at Appendix E which contains a table listing the various symbols\nusedwithinouralgorithmsandtheirmeaning. Onekeywordthatwewouldlike\nto point out here is yield . You can think of yield in the same light as return .\nThe return keywordcausesthemethodtoexitandreturnscontroltothecaller,\nwhereas yield returns each value to the caller. With yield control only returns\nto the caller when all values to return to the caller have been exhausted.\nCHAPTER 1. INTRODUCTION 7",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 5,
      "heading": "1.6 Testing",
      "text": "All the data structures and algorithms have been tested using a minimised test\ndriven development style on paper to flesh out the pseudocode algorithm. We\nthen transcribe these tests into unit tests satisfying them one by one. When\nall the test cases have been progressively satisfied we consider that algorithm\nsuitably tested.\nFor the most part algorithms have fairly obvious cases which need to be\nsatisfied. Some however have many areas which can prove to be more complex\ntosatisfy. Withsuchalgorithmswewillpointoutthetestcaseswhicharetricky\nandthecorrespondingportionsofpseudocodewithinthealgorithmthatsatisfy\nthat respective case.\nAs you become more familiar with the actual problem you will be able to\nintuitively identify areas which may cause problems for your algorithms imple-\nmentation. Thisinsomecaseswillyieldanoverwhelminglistofconcernswhich\nwill hinder your ability to design an algorithm greatly. When you are bom-\nbarded with such a vast amount of concerns look at the overall problem again\nandsub-dividetheproblemintosmallerproblems. Solvingthesmallerproblems\nand then composing them is a far easier task than clouding your mind with too\nmany little details.\nThe only type of testing that we use in the implementation of all that is\nprovided in this book are unit tests. Because unit tests contribute such a core\npiece of creating somewhat more stable software we invite the reader to view\nAppendix D which describes testing in more depth.\n\nThis book doesn’t provide any code specifically aligned with it, however we do\nactively maintain an open source project that houses a C# implementation of\nallthepseudocodelisted. Theprojectisnamed DataStructuresandAlgorithms\n(DSA) and can be found at http://codeplex.com/dsa .\n\nWe have just a few final messages to the reader that we hope you digest before\nyou embark on reading this book:\n1. Understand how the algorithm works first in an abstract sense; and\n2. Always work through the algorithms on paper to understand how they\nachieve their outcome\nIfyoualwaysfollowthesekeypoints, youwillgetthemostoutofthisbook.\nAll readers are encouraged to provide suggestions, feature requests, and bugs so we can\nfurtherimproveourimplementations.\n\nLinked lists can be thought of from a high level perspective as being a series\nof nodes. Each node has at least a single pointer to the next node, and in the\nlast node’s case a null pointer representing that there are no more nodes in the\nlinked list.\nIn DSA our implementations of linked lists always maintain head and tail\npointers so that insertion at either the head or tail of the list is a constant\ntime operation. Random insertion is excluded from this and will be a linear\noperation. As such, linked lists in DSA have the following characteristics:\n1. Insertion is O (1)\n2. Deletion is O ( n )\n3. Searching is O ( n )\nOut of the three operations the one that stands out is that of insertion. In\nDSA we chose to always maintain pointers (or more aptly references) to the\nnode(s) at the head and tail of the linked list and so performing a traditional\ninsertion to either the front or back of the linked list is an O (1) operation. An\nexception to this rule is performing an insertion before a node that is neither\nthe head nor tail in a singly linked list. When the node we are inserting before\nis somewhere in the middle of the linked list (known as random insertion) the\ncomplexity is O ( n ). In order to add before the designated node we need to\ntraverse the linked list to find that node’s current predecessor. This traversal\nyields an O ( n ) run time.\nThisdatastructureistrivial, butlinkedlistshaveafewkeypointswhichat\ntimes make them very attractive:\n1. thelistisdynamicallyresized,thusitincursnocopypenaltylikeanarray\nor vector would eventually incur; and\n2. insertion is O (1).",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 6,
      "heading": "2.1 Singly Linked List",
      "text": "Singly linked lists are one of the most primitive data structures you will find in\nthis book. Each node that makes up a singly linked list consists of a value, and\na reference to the next node (if any) in the list.\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\nCHAPTER 2. LINKED LISTS 10\nFigure 2.1: Singly linked list node\nFigure 2.2: A singly linked list populated with integers\n2.1.1 Insertion\nIn general when people talk about insertion with respect to linked lists of any\nform they implicitly refer to the adding of a node to the tail of the list. When\nyou use an API like that of DSA and you see a general purpose method that\naddsanodetothelist,youcanassumethatyouareaddingthenodetothetail\nof the list not the head.\nAdding a node to a singly linked list has only two cases:\n1. head = ∅ in which case the node we are adding is now both the head and\ntail of the list; or\n2. we simply need to append our node onto the end of the list updating the\ntail reference appropriately.\n1) algorithm Add( value )\n2) Pre: value is the value to add to the list\n3) Post: value has been placed at the tail of the list\n4) n ← node( value )\n5) if head = ∅\n6) head ← n\n7) tail ← n\n8) else\n9) tail .Next ← n\n10) tail ← n\n11) end if\n12) end Add\nAs an example of the previous algorithm consider adding the following se-\nquence of integers to the list: 1, 45, 60, and 12, the resulting list is that of\nFigure 2.2.\n2.1.2 Searching\nSearching a linked list is straightforward: we simply traverse the list checking\nthe value we are looking for with the value of each node in the linked list. The\nalgorithmlistedinthissectionisverysimilartothatusedfortraversalin § 2.1.4.\nCHAPTER 2. LINKED LISTS 11\n1) algorithm Contains( head , value )\n2) Pre: head is the head node in the list\n3) value is the value to search for\n4) Post: the item is either in the linked list, true; otherwise false\n5) n ← head\n6) while n [?] = ∅ and n .Value [?] = value\n7) n ← n .Next\n8) end while\n9) if n = ∅\n10) return false\n11) end if\n12) return true\n13) end Contains\n2.1.3 Deletion\nDeleting a node from a linked list is straightforward but there are a few cases\nwe need to account for:\n1. the list is empty; or\n2. the node to remove is the only node in the linked list; or\n3. we are removing the head node; or\n4. we are removing the tail node; or\n5. the node to remove is somewhere in between the head and tail; or\n6. the item to remove doesn’t exist in the linked list\nThe algorithm whose cases we have described will remove a node from any-\nwherewithinalistirrespectiveofwhetherthenodeisthe head etc. Ifyouknow\nthat items will only ever be removed from the head or tail of the list then you\ncan create much more concise algorithms. In the case of always removing from\nthe front of the linked list deletion becomes an O (1) operation.\nCHAPTER 2. LINKED LISTS 12\n1) algorithm Remove( head , value )\n2) Pre: head is the head node in the list\n3) value is the value to remove from the list\n4) Post: value is removed from the list, true; otherwise false\n5) if head = ∅\n6) // case 1\n7) return false\n8) end if\n9) n ← head\n10) if n .Value = value\n11) if head = tail\n12) // case 2\n13) head ←∅\n14) tail ←∅\n15) else\n16) // case 3\n17) head ← head .Next\n18) end if\n19) return true\n20) end if\n21) while n .Next [?] = ∅ and n .Next.Value [?] = value\n22) n ← n .Next\n23) end while\n24) if n .Next [?] = ∅\n25) if n .Next = tail\n26) // case 4\n27) tail ← n\n28) end if\n29) // this is only case 5 if the conditional on line 25 was false\n30) n .Next ← n .Next.Next\n31) return true\n32) end if\n33) // case 6\n34) return false\n35) end Remove\n2.1.4 Traversing the list\nTraversing a singly linked list is the same as that of traversing a doubly linked\nlist (defined in § 2.2). You start at the head of the list and continue until you\ncome across a node that is ∅ . The two cases are as follows:\n1. node = ∅ , we have exhausted all nodes in the linked list; or\n2. we must update the node reference to be node .Next.\nThe algorithm described is a very simple one that makes use of a simple\nwhile loop to check the first case.\nCHAPTER 2. LINKED LISTS 13\n1) algorithm Traverse( head )\n2) Pre: head is the head node in the list\n3) Post: the items in the list have been traversed\n4) n ← head\n5) while n [?] =0\n6) yield n .Value\n7) n ← n .Next\n8) end while\n9) end Traverse\n2.1.5 Traversing the list in reverse order\nTraversing a singly linked list in a forward manner (i.e. left to right) is simple\nasdemonstratedin § 2.1.4. However, whatifwewantedtotraversethenodesin\nthe linked list in reverse order for some reason? The algorithm to perform such\na traversal is very simple, and just like demonstrated in § 2.1.3 we will need to\nacquire a reference to the predecessor of a node, even though the fundamental\ncharacteristics of the nodes that make up a singly linked list make this an\nexpensiveoperation. Foreachnode,findingitspredecessorisan O ( n )operation,\nsooverthecourseoftraversingthewholelistbackwardsthecostbecomes O ( n ).\nFigure2.3depictsthefollowingalgorithmbeingappliedtoalinkedlistwith\nthe integers 5, 10, 1, and 40.\n1) algorithm ReverseTraversal( head , tail )\n2) Pre: head and tail belong to the same list\n3) Post: the items in the list have been traversed in reverse order\n4) if tail [?] = ∅\n5) curr ← tail\n6) while curr [?] = head\n7) prev ← head\n8) while prev .Next [?] = curr\n9) prev ← prev .Next\n10) end while\n11) yield curr .Value\n12) curr ← prev\n13) end while\n14) yield curr .Value\n15) end if\n16) end ReverseTraversal\nThis algorithm is only of real interest when we are using singly linked lists,\nas you will soon see that doubly linked lists (defined in § 2.2) make reverse list\ntraversal simple and efficient, as shown in § 2.2.3.",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 7,
      "heading": "2.2 Doubly Linked List",
      "text": "Doubly linked lists are very similar to singly linked lists. The only difference is\nthat each node has a reference to both the next and previous nodes in the list.\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\n[TABLE:\n | \n]\nCHAPTER 2. LINKED LISTS 14\nFigure 2.3: Reverse traveral of a singly linked list\nFigure 2.4: Doubly linked list node\n[TABLE:\n |  | \n]\n[TABLE:\n |  | \n]\n[TABLE:\n |  | \n]\n[TABLE:\n |  | \n]\nCHAPTER 2. LINKED LISTS 15\nThe following algorithms for the doubly linked list are exactly the same as\nthose listed previously for the singly linked list:\n1. Searching (defined in § 2.1.2)\n2. Traversal (defined in § 2.1.4)\n2.2.1 Insertion\nThe only major difference between the algorithm in § 2.1.1 is that we need to\nremember to bind the previous pointer of n to the previous tail node if n was\nnot the first node to be inserted into the list.\n1) algorithm Add( value )\n2) Pre: value is the value to add to the list\n3) Post: value has been placed at the tail of the list\n4) n ← node( value )\n5) if head = ∅\n6) head ← n\n7) tail ← n\n8) else\n9) n .Previous ← tail\n10) tail .Next ← n\n11) tail ← n\n12) end if\n13) end Add\nFigure 2.5 shows the doubly linked list after adding the sequence of integers\ndefined in § 2.1.1.\nFigure 2.5: Doubly linked list populated with integers\n2.2.2 Deletion\nAs you may of guessed the cases that we use for deletion in a doubly linked\nlist are exactly the same as those defined in § 2.1.3. Like insertion we have the\nadded task of binding an additional reference ( Previous ) to the correct value.\nCHAPTER 2. LINKED LISTS 16\n1) algorithm Remove( head , value )\n2) Pre: head is the head node in the list\n3) value is the value to remove from the list\n4) Post: value is removed from the list, true; otherwise false\n5) if head = ∅\n6) return false\n7) end if\n8) if value = head .Value\n9) if head = tail\n10) head ←∅\n11) tail ←∅\n12) else\n13) head ← head .Next\n14) head .Previous ←∅\n15) end if\n16) return true\n17) end if\n18) n ← head .Next\n19) while n [?] = ∅ and value [?] = n .Value\n20) n ← n .Next\n21) end while\n22) if n = tail\n23) tail ← tail .Previous\n24) tail .Next ←∅\n25) return true\n26) else if n [?] = ∅\n27) n .Previous.Next ← n .Next\n28) n .Next.Previous ← n .Previous\n29) return true\n30) end if\n31) return false\n32) end Remove\n2.2.3 Reverse Traversal\nSinglylinkedlistshaveaforwardonlydesign,whichiswhythereversetraversal\nalgorithmdefinedin § 2.1.5requiredsomecreativeinvention. Doublylinkedlists\nmake reverse traversal as simple as forward traversal (defined in § 2.1.4) except\nthatwestartatthetailnodeandupdatethepointersintheoppositedirection.\nFigure 2.6 shows the reverse traversal algorithm in action.\n[TABLE:\n |  | \n]\n[TABLE:\n |  | \n]\n[TABLE:\n |  | \n]\n[TABLE:\n |  | \n]\nCHAPTER 2. LINKED LISTS 17\nFigure 2.6: Doubly linked list reverse traversal\n1) algorithm ReverseTraversal( tail )\n2) Pre: tail is the tail node of the list to traverse\n3) Post: the list has been traversed in reverse order\n4) n ← tail\n5) while n [?] = ∅\n6) yield n .Value\n7) n ← n .Previous\n8) end while\n9) end ReverseTraversal\n\nLinked lists are good to use when you have an unknown number of items to\nstore. Using a data structure like an array would require you to specify the size\nup front; exceeding that size involves invoking a resizing algorithm which has\na linear run time. You should also use linked lists when you will only remove\nnodes at either the head or tail of the list to maintain a constant run time.\nThis requires maintaining pointers to the nodes at the head and tail of the list\nbut the memory overhead will pay for itself if this is an operation you will be\nperforming many times.\nWhat linked lists are not very good for is random insertion, accessing nodes\nby index, and searching. At the expense of a little memory (in most cases 4\nbytes would suffice), and a few more read/writes you could maintain a count\nvariable that tracks how many items are contained in the list so that accessing\nsuch a primitive property is a constant operation - you just need to update\ncount during the insertion and deletion algorithms.\nSingly linked lists should be used when you are only performing basic in-\nsertions. In general doubly linked lists are more accommodating for non-trivial\noperations on a linked list.\nWe recommend the use of a doubly linked list when you require forwards\nand backwards traversal. For the most cases this requirement is present. For\nexample, consider a token stream that you want to parse in a recursive descent\nfashion. Sometimes you will have to backtrack in order to create the correct\nparse tree. In this scenario a doubly linked list is best as its design makes\nbi-directional traversal much simpler and quicker than that of a singly linked\nCHAPTER 2. LINKED LISTS 18\nlist.",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 8,
      "heading": "Binary Search Tree",
      "text": "Binarysearchtrees(BSTs)areverysimpletounderstand. Westartwitharoot\nnode with value x , where the left subtree of x contains nodes with values < x\nand the right subtree contains nodes whose values are ≥ x . Each node follows\nthe same rules with respect to nodes in their left and right subtrees.\nBSTsareofinterestbecausetheyhaveoperationswhicharefavourablyfast:\ninsertion,lookup,anddeletioncanallbedonein O ( log n )time. Itisimportant\nto note that the O ( log n ) times for these operations can only be attained if\nthe BST is reasonably balanced; for a tree data structure with self balancing\nproperties see AVL tree defined in § 7).\nIn the following examples you can assume, unless used as a parameter alias\nthat root is a reference to the root node of the tree.\n14 31\n7 17\nFigure 3.1: Simple unbalanced binary search tree\nCHAPTER 3. BINARY SEARCH TREE 20\n\nAs mentioned previously insertion is an O ( log n ) operation provided that the\ntree is moderately balanced.\n1) algorithm Insert( value )\n2) Pre: value has passed custom type checks for type T\n3) Post: value has been placed in the correct location in the tree\n4) if root = ∅\n5) root ← node( value )\n6) else\n7) InsertNode( root , value )\n8) end if\n9) end Insert\n1) algorithm InsertNode( current , value )\n2) Pre: current is the node to start from\n3) Post: value has been placed in the correct location in the tree\n4) if value<current .Value\n5) if current .Left = ∅\n6) current .Left ← node( value )\n7) else\n8) InsertNode( current .Left, value )\n9) end if\n10) else\n11) if current .Right = ∅\n12) current .Right ← node( value )\n13) else\n14) InsertNode( current .Right, value )\n15) end if\n16) end if\n17) end InsertNode\nThe insertion algorithm is split for a good reason. The first algorithm (non-\nrecursive) checks a very core base case - whether or not the tree is empty. If\nthe tree is empty then we simply create our root node and finish. In all other\ncases we invoke the recursive InsertNode algorithm which simply guides us to\nthe first appropriate place in the tree to put value . Note that at each stage we\nperform a binary chop: we either choose to recurse into the left subtree or the\nrightbycomparingthenewvaluewiththatofthecurrentnode. Foranytotally\nordered type, no value can simultaneously satisfy the conditions to place it in\nboth subtrees.\nCHAPTER 3. BINARY SEARCH TREE 21\n\nSearchingaBSTisevensimplerthaninsertion. Thepseudocodeisself-explanatory\nbut we will look briefly at the premise of the algorithm nonetheless.\nWehavetalkedpreviouslyaboutinsertion,wegoeitherleftorrightwiththe\nright subtree containing values that are ≥ x where x is the value of the node\nwe are inserting. When searching the rules are made a little more atomic and\nat any one time we have four cases to consider:\n1. the root = ∅ in which case value is not in the BST; or\n2. root .Value = value in which case value is in the BST; or\n3. value<root .Value, we must inspect the left subtree of root for value ; or\n4. value>root .Value, we must inspect the right subtree of root for value .\n1) algorithm Contains( root , value )\n2) Pre: root is the root node of the tree, value is what we would like to locate\n3) Post: value is either located or not\n4) if root = ∅\n5) return false\n6) end if\n7) if root .Value = value\n8) return true\n9) else if value<root .Value\n10) return Contains( root .Left, value )\n11) else\n12) return Contains( root .Right, value )\n13) end if\n14) end Contains\nCHAPTER 3. BINARY SEARCH TREE 22",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 9,
      "heading": "3.3 Deletion",
      "text": "Removing a node from a BST is fairly straightforward, with four cases to con-\nsider:\n1. the value to remove is a leaf node; or\n2. the value to remove has a right subtree, but no left subtree; or\n3. the value to remove has a left subtree, but no right subtree; or\n4. the value to remove has both a left and right subtree in which case we\npromote the largest value in the left subtree.\nThere is also an implicit fifth case whereby the node to be removed is the\nonly node in the tree. This case is already covered by the first, but should be\nnoted as a possibility nonetheless.\nOf course in a BST a value may occur more than once. In such a case the\nfirst occurrence of that value in the BST will be removed.\n#4: Right subtree\nand left subtree\n#3: Left subtree\n14 31\nno right subtree\n#2: Right subtree\nno left subtree\n#1: Leaf Node 9\nFigure 3.2: binary search tree deletion cases\nThe Remove algorithm given below relies on two further helper algorithms\nnamed FindParent , and FindNode which are described in § 3.4 and § 3.5 re-\nspectively.\nCHAPTER 3. BINARY SEARCH TREE 23\n1) algorithm Remove( value )\n2) Pre: value is the value of the node to remove, root is the root node of the BST\n3) Count is the number of items in the BST\n3) Post: node with value is removed if found in which case yields true, otherwise false\n4) nodeToRemove ← FindNode( value )\n5) if nodeToRemove = ∅\n6) return false // value not in BST\n7) end if\n8) parent ← FindParent( value )\n9) if Count =1\n10) root ←∅ // we are removing the only node in the BST\n11) else if nodeToRemove .Left = ∅ and nodeToRemove .Right = null\n12) // case #1\n13) if nodeToRemove .Value <parent .Value\n14) parent .Left ←∅\n15) else\n16) parent .Right ←∅\n17) end if\n18) else if nodeToRemove .Left = ∅ and nodeToRemove .Right [?] = ∅\n19) // case # 2\n20) if nodeToRemove .Value <parent .Value\n21) parent .Left ← nodeToRemove .Right\n22) else\n23) parent .Right ← nodeToRemove .Right\n24) end if\n25) else if nodeToRemove .Left [?] = ∅ and nodeToRemove .Right = ∅\n26) // case #3\n27) if nodeToRemove .Value <parent .Value\n28) parent .Left ← nodeToRemove .Left\n29) else\n30) parent .Right ← nodeToRemove .Left\n31) end if\n32) else\n33) // case #4\n34) largestValue ← nodeToRemove .Left\n35) while largestValue .Right [?] = ∅\n36) // find the largest value in the left subtree of nodeToRemove\n37) largestValue ← largestValue .Right\n38) end while\n39) // set the parents’ Right pointer of largestValue to ∅\n40) FindParent( largestValue .Value).Right ←∅\n41) nodeToRemove .Value ← largestValue .Value\n42) end if\n43) Count ← Count − 1\n44) return true\n45) end Remove\nCHAPTER 3. BINARY SEARCH TREE 24\n\nThe purpose of this algorithm is simple - to return a reference (or pointer) to\nthe parent node of the one with the given value. We have found that such an\nalgorithm is very useful, especially when performing extensive tree transforma-\ntions.\n1) algorithm FindParent( value , root )\n2) Pre: value is the value of the node we want to find the parent of\n3) root is the root node of the BST and is != ∅\n4) Post: a reference to the parent node of value if found; otherwise ∅\n5) if value = root .Value\n6) return ∅\n7) end if\n8) if value<root .Value\n9) if root .Left = ∅\n10) return ∅\n11) else if root .Left.Value = value\n12) return root\n13) else\n14) return FindParent( value , root .Left)\n15) end if\n16) else\n17) if root .Right = ∅\n18) return ∅\n19) else if root .Right.Value = value\n20) return root\n21) else\n22) return FindParent( value , root .Right)\n23) end if\n24) end if\n25) end FindParent\nA special case in the above algorithm is when the specified value does not\nexistintheBST,inwhichcasewereturn ∅ . Callerstothisalgorithmmusttake\naccount of this possibility unless they are already certain that a node with the\nspecified value exists.\n\nThisalgorithmisverysimilarto § 3.4,butinsteadofreturningareferencetothe\nparent of the node with the specified value, it returns a reference to the node\nitself. Again, ∅ is returned if the value isn’t found.\nCHAPTER 3. BINARY SEARCH TREE 25\n1) algorithm FindNode( root , value )\n2) Pre: value is the value of the node we want to find the parent of\n3) root is the root node of the BST\n4) Post: a reference to the node of value if found; otherwise ∅\n5) if root = ∅\n6) return ∅\n7) end if\n8) if root .Value = value\n9) return root\n10) else if value<root .Value\n11) return FindNode( root .Left, value )\n12) else\n13) return FindNode( root .Right, value )\n14) end if\n15) end FindNode\nAstute readers will have noticed that the FindNode algorithm is exactly the\nsame as the Contains algorithm (defined in § 3.2) with the modification that\nwe are returning a reference to a node not true or false . Given FindNode ,\nthe easiest way of implementing Contains is to call FindNode and compare the\nreturn value with ∅ .\n\nTo find the smallest value in a BST you simply traverse the nodes in the left\nsubtree of the BST always going left upon each encounter with a node, termi-\nnatingwhenyoufindanodewithnoleftsubtree. Theoppositeisthecasewhen\nfindingthelargestvalueintheBST.Bothalgorithmsareincrediblysimple,and\nare listed simply for completeness.\nThebasecaseinboth FindMin ,and FindMax algorithmsiswhentheLeft\n( FindMin ), or Right ( FindMax ) node references are ∅ in which case we have\nreached the last node.\n1) algorithm FindMin( root )\n2) Pre: root is the root node of the BST\n3) root [?] = ∅\n4) Post: the smallest value in the BST is located\n5) if root .Left = ∅\n6) return root .Value\n7) end if\n8) FindMin( root .Left)\n9) end FindMin\nCHAPTER 3. BINARY SEARCH TREE 26\n1) algorithm FindMax( root )\n2) Pre: root is the root node of the BST\n3) root [?] = ∅\n4) Post: the largest value in the BST is located\n5) if root .Right = ∅\n6) return root .Value\n7) end if\n8) FindMax( root .Right)\n9) end FindMax",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 10,
      "heading": "3.7 Tree Traversals",
      "text": "There are various strategies which can be employed to traverse the items in a\ntree; the choice of strategy depends on which node visitation order you require.\nIn this section we will touch on the traversals that DSA provides on all data\nstructures that derive from BinarySearchTree .\n3.7.1 Preorder\nWhenusingthepreorderalgorithm,youvisittherootfirst,thentraversetheleft\nsubtree and finally traverse the right subtree. An example of preorder traversal\nis shown in Figure 3.3.\n1) algorithm Preorder( root )\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in preorder\n4) if root [?] = ∅\n5) yield root .Value\n6) Preorder( root .Left)\n7) Preorder( root .Right)\n8) end if\n9) end Preorder\n3.7.2 Postorder\nThis algorithm is very similar to that described in § 3.7.1, however the value\nof the node is yielded after traversing both subtrees. An example of postorder\ntraversal is shown in Figure 3.4.\n1) algorithm Postorder( root )\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in postorder\n4) if root [?] = ∅\n5) Postorder( root .Left)\n6) Postorder( root .Right)\n7) yield root .Value\n8) end if\n9) end Postorder\nCHAPTER 3. BINARY SEARCH TREE 27\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.3: Preorder visit binary search tree example\nCHAPTER 3. BINARY SEARCH TREE 28\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.4: Postorder visit binary search tree example\nCHAPTER 3. BINARY SEARCH TREE 29\n3.7.3 Inorder\nAnothervariationofthealgorithmsdefinedin § 3.7.1and § 3.7.2isthatofinorder\ntraversal where the value of the current node is yielded in between traversing\ntheleftsubtreeandtherightsubtree. Anexampleofinordertraversalisshown\nin Figure 3.5.\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.5: Inorder visit binary search tree example\n1) algorithm Inorder( root )\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in inorder\n4) if root [?] = ∅\n5) Inorder( root .Left)\n6) yield root .Value\n7) Inorder( root .Right)\n8) end if\n9) end Inorder\nOne of the beauties of inorder traversal is that values are yielded in their\ncomparison order. In other words, when traversing a populated BST with the\ninorder strategy, the yielded sequence would have property x ≤ x ∀ i .\ni i +1\nCHAPTER 3. BINARY SEARCH TREE 30\n3.7.4 Breadth First\nTraversing a tree in breadth first order yields the values of all nodes of a par-\nticular depth in the tree before any deeper ones. In other words, given a depth\nd we would visit the values of all nodes at d in a left to right fashion, then we\nwould proceed to d +1 and so on until we hade no more nodes to visit. An\nexample of breadth first traversal is shown in Figure 3.6.\nTraditionally breadth first traversal is implemented using a list (vector, re-\nsizeablearray, etc)tostorethevaluesofthenodesvisitedinbreadthfirstorder\nand then a queue to store those nodes that have yet to be visited.\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.6: Breadth First visit binary search tree example\nCHAPTER 3. BINARY SEARCH TREE 31\n1) algorithm BreadthFirst( root )\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in breadth first order\n4) q ← queue\n5) while root [?] = ∅\n6) yield root .Value\n7) if root .Left [?] = ∅\n8) q .Enqueue( root .Left)\n9) end if\n10) if root .Right [?] = ∅\n11) q .Enqueue( root .Right)\n12) end if\n13) if ! q .IsEmpty()\n14) root ← q .Dequeue()\n15) else\n16) root ←∅\n17) end if\n18) end while\n19) end BreadthFirst\n\nAbinarysearchtreeisagoodsolutionwhenyouneedtorepresenttypesthatare\norderedaccordingtosomecustomrulesinherenttothattype. Withlogarithmic\ninsertion, lookup, and deletion it is very effecient. Traversal remains linear, but\nthere are many ways in which you can visit the nodes of a tree. Trees are\nrecursive data structures, so typically you will find that many algorithms that\noperate on a tree are recursive.\nTheruntimespresentedinthischapterarebasedonaprettybigassumption\n- that the binary search tree’s left and right subtrees are reasonably balanced.\nWe can only attain logarithmic run times for the algorithms presented earlier\nwhen this is true. A binary search tree does not enforce such a property, and\nthe run times for these operations on a pathologically unbalanced tree become\nlinear: such a tree is effectively just a linked list. Later in § 7 we will examine\nan AVL tree that enforces self-balancing properties to help attain logarithmic\nrun times.\n\nAheapcanbethoughtofasasimpletreedatastructure,howeveraheapusually\nemploys one of two strategies:\n1. min heap; or\n2. max heap\nEach strategy determines the properties of the tree and its values. If you\nweretochoosetheminheapstrategytheneachparentnodewouldhaveavalue\nthat is ≤ than its children. For example, the node at the root of the tree will\nhave the smallest value in the tree. The opposite is true for the max heap\nstrategy. In this book you should assume that a heap employs the min heap\nstrategy unless otherwise stated.\nUnlikeothertreedatastructuresliketheonedefinedin § 3aheapisgenerally\nimplemented as an array rather than a series of nodes which each have refer-\nences to other nodes. The nodes are conceptually the same, however, having at\nmost two children. Figure 4.1 shows how the tree (not a heap data structure)\n(127(32)6(9 ))wouldberepresentedasanarray. ThearrayinFigure4.1isa\nresult of simply adding values in a top-to-bottom, left-to-right fashion. Figure\n4.2 shows arrows to the direct left and right child of each value in the array.\nThischapterisverymuchcentredaroundthenotionofrepresentingatreeas\nan array and because this property is key to understanding this chapter Figure\n4.3 shows a step by step process to represent a tree data structure as an array.\nIn Figure 4.3 you can assume that the default capacity of our array is eight.\nUsingjustanarrayisoftennotsufficientaswehavetobeupfrontaboutthe\nsizeofthearraytousefortheheap. Oftentheruntimebehaviourofaprogram\ncan be unpredictable when it comes to the size of its internal data structures,\nsoweneedtochooseamoredynamicdatastructurethatcontainsthefollowing\nproperties:\n1. we can specify an initial size of the array for scenarios where we know the\nupper storage limit required; and\n2. the data structure encapsulates resizing algorithms to grow the array as\nrequired at run time\n[TABLE:\n |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  | \n]\nCHAPTER 4. HEAP 33\nFigure 4.1: Array representation of a simple tree data structure\nFigure4.2: Directchildrenofthenodesinanarrayrepresentationofatreedata\nstructure\n1. Vector\n2. ArrayList\n3. List\nFigure 4.1 does not specify how we would handle adding null references to\nthe heap. This varies from case to case; sometimes null values are prohibited\nentirely; in other cases we may treat them as being smaller than any non-null\nvalue, or indeed greater than any non-null value. You will have to resolve this\nambiguityyourselfhavingstudiedyourrequirements. Forthesakeofclaritywe\nwill avoid the issue by prohibiting null values.\nBecause we are using an array we need some way to calculate the index of a\nparent node, and the children of a node. The required expressions for this are\ndefined as follows for a node at index :\n1. ( index − 1)/2 (parent index)\n2. 2 ∗ index +1 (left child)\n3. 2 ∗ index +2 (right child)\nIn Figure 4.4 a) represents the calculation of the right child of 12 (2 ∗ 0+2);\nand b) calculates the index of the parent of 3 ((3 − 1)/2).",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    },
    {
      "segment_id": 11,
      "heading": "4.1 Insertion",
      "text": "Designing an algorithm for heap insertion is simple, but we must ensure that\nheap order is preserved after each insertion. Generally this is a post-insertion\noperation. Insertingavalueintothenextfreeslotinanarrayissimple: wejust\nneedtokeeptrackofthenextfreeindexinthearrayasacounter,andincrement\nit after each insertion. Inserting our value into the heap is the first part of the\nalgorithm;thesecondisvalidatingheaporder. Inthecaseofmin-heapordering\nthis requires us to swap the values of a parent and its child if the value of the\nchild is < the value of its parent. We must do this for each subtree containing\nthe value we just inserted.\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\nCHAPTER 4. HEAP 34\nFigure 4.3: Converting a tree data structure to its array counterpart\n[TABLE:\n |  |  |  |  | \n]\nCHAPTER 4. HEAP 35\nFigure 4.4: Calculating node properties\nThe run time efficiency for heap insertion is O ( log n ). The run time is a\nby product of verifying heap order as the first part of the algorithm (the actual\ninsertion into the array) is O (1).\nFigure 4.5 shows the steps of inserting the values 3, 9, 12, 7, and 1 into a\nmin-heap.\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\n[TABLE:\n |  |  |  |  |  |  | \n]\nCHAPTER 4. HEAP 36\nFigure 4.5: Inserting values into a min-heap\nCHAPTER 4. HEAP 37\n1) algorithm Add( value )\n2) Pre: value is the value to add to the heap\n3) Count is the number of items in the heap\n4) Post: the value has been added to the heap\n5) heap [Count] ← value\n6) Count ← Count +1\n7) MinHeapify()\n8) end Add\n1) algorithm MinHeapify()\n2) Pre: Count is the number of items in the heap\n3) heap is the array used to store the heap items\n4) Post: the heap has preserved min heap ordering\n5) i ← Count − 1\n6) while i> 0 and heap [ i ] <heap [( i − 1)/2]\n7) Swap( heap [ i ], heap [( i − 1)/2]\n8) i ← ( i − 1)/2\n9) end while\n10) end MinHeapify\nThe design of the MaxHeapify algorithm is very similar to that of the Min-\nHeapify algorithm, the only difference is that the < operator in the second\ncondition of entering the while loop is changed to > .\n\nJust as for insertion, deleting an item involves ensuring that heap ordering is\npreserved. The algorithm for deletion has three steps:\n1. find the index of the value to delete\n2. put the last value in the heap at the index location of the item to delete\n3. verify heap ordering for each subtree which used to include the value\nCHAPTER 4. HEAP 38\n1) algorithm Remove( value )\n2) Pre: value is the value to remove from the heap\n3) left , and right are updated alias’ for 2 ∗ index +1, and 2 ∗ index +2 respectively\n4) Count is the number of items in the heap\n5) heap is the array used to store the heap items\n6) Post: value is located in the heap and removed, true; otherwise false\n7) // step 1\n8) index ← FindIndex( heap , value )\n9) if index< 0\n10) return false\n11) end if\n12) Count ← Count − 1\n13) // step 2\n14) heap [ index ] ← heap [Count]\n15) // step 3\n16) while left< Count and heap [ index ] >heap [ left ] or heap [ index ] >heap [ right ]\n17) // promote smallest key from subtree\n18) if heap [ left ] <heap [ right ]\n19) Swap( heap , left , index )\n20) index ← left\n21) else\n22) Swap( heap , right , index )\n23) index ← right\n24) end if\n25) end while\n26) return true\n27) end Remove\nFigure 4.6 shows the Remove algorithm visually, removing 1 from a heap\ncontaining the values 1, 3, 9, 12, and 13. In Figure 4.6 you can assume that we\nhavespecifiedthatthebackingarrayoftheheapshouldhaveaninitialcapacity\nof eight.\nPleasenotethatinourdeletionalgorithmthatwedon’tdefaulttheremoved\nvalue in the heap array. If you are using a heap for reference types, i.e. objects\nthatareallocatedonaheapyouwillwanttofreethatmemory. Thisisimportant\nin both unmanaged, and managed languages. In the latter we will want to null\nthat empty hole so that the garbage collector can reclaim that memory. If we\nwere to not null that hole then the object could still be reached and thus won’t\nbe garbage collected.",
      "segment_type": "exercise",
      "scores": {
        "goal_focus": 0,
        "text_readability": 0,
        "pedagogical_clarity": 0,
        "prerequisite_alignment": 0,
        "fluidity_continuity": 0,
        "structural_usability": 0,
        "example_concreteness": 0,
        "example_coherence": 0,
        "business_relevance": 0,
        "instructional_alignment": 0
      },
      "reasoning": {
        "goal_focus_rationale": "N/A",
        "text_readability_rationale": "N/A",
        "pedagogical_clarity_rationale": "N/A",
        "prerequisite_alignment_rationale": "N/A",
        "fluidity_continuity_rationale": "N/A",
        "structural_usability_rationale": "N/A",
        "example_concreteness_rationale": "N/A",
        "example_coherence_rationale": "N/A",
        "business_relevance_rationale": "N/A",
        "instructional_alignment_rationale": "N/A"
      }
    }
  ],
  "evaluation_meta": {
    "model_used": "Gemini-2.5-Flash",
    "timestamp": "2026-02-25T10:55:47.443572+00:00",
    "prompt_version": "1.1",
    "total_segments": 11,
    "instructional_segments_scored": 1,
    "excluded_segments": 10
  }
}